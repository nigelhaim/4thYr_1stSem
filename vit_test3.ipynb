{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Focused on training using the ground truth (perfect RPN) bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd  \n",
    "from project.dataset import Dataset, VALDODataset\n",
    "from project.preprocessing import NiftiToTensorTransform, get_transform\n",
    "from project.utils import collatev2, compute_statistics \n",
    "from torch.utils.data import DataLoader\n",
    "from project.model ISAVIT\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib.pyplot as plt\n",
    "from project.model.feeer import Feeder\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime as dtt\n",
    "import os\n",
    "\n",
    "path = 'logs'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "dte = dtt.now().strftime('%b_%d_%Y_%H%M%S')\n",
    "\n",
    "logger = logging.getLogger('nigel')\n",
    "fh = logging.FileHandler(f'logs/ViT{dte}.log')\n",
    "formatter = logging.Formatter(\n",
    "    '%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(fh)\n",
    "\n",
    "dte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 300\n",
    "patch_size = 16\n",
    "\n",
    "config = {\n",
    "    'model': ISAVIT(\n",
    "        d_model=512,\n",
    "        patch_size=patch_size,\n",
    "        dim_ff=1600\n",
    "    ).to(device),\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'device': device,\n",
    "    'epochs': 10,\n",
    "    'loss': nn.BCEWithLogitsLoss(),\n",
    "    # 'loss': nn.MSELoss(),\n",
    "    'lr': 0.0001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = get_transform(\n",
    "    height=patch_size,\n",
    "    width=patch_size,\n",
    "    p=1.0,\n",
    "    rpn_mode=False\n",
    ")\n",
    "\n",
    "feeder = Feeder(resize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset()\n",
    "\n",
    "data = pd.read_csv('targets.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.query('has_microbleed_slice == 1').reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataLoader` Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr(data, col):\n",
    "    q3 = data[col].quantile(0.75)\n",
    "    q1 = data[col].quantile(0.25)\n",
    "    iqr = q3-q1\n",
    "    new = data[(data[col] < (q3 + 1.5*iqr)) & (data[col] > (q1 - 1.5*iqr))]\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_loaders(data,\n",
    "                 cohort,\n",
    "                 batch_size,\n",
    "                 test_size=0.2,\n",
    "                 random_state=12,\n",
    "                 target_shape=(300, 300),\n",
    "                 rpn_mode=True,\n",
    "                 logger=None\n",
    "                ):\n",
    "    data = data[data.cohort == cohort]\n",
    "    # data = iqr(data, 'max_value')\n",
    "    \n",
    "    s = f'Creating loaders for Cohort {cohort}\\n'\n",
    "\n",
    "    data_train, data_test = train_test_split(\n",
    "        data,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    s += f'TRAIN & TEST: {data_train.shape, data_test.shape}\\n'\n",
    "\n",
    "    paths = data_train.mri.unique().tolist()\n",
    "    s += f'Total Unique MRI Samples in data_train: {len(paths)}\\n'\n",
    "    \n",
    "    global_min, global_max = compute_statistics(paths)\n",
    "    s += f'GLOBAL MIN & MAX {global_min, global_max}\\n'\n",
    "\n",
    "    transform = NiftiToTensorTransform(\n",
    "        target_shape=target_shape,\n",
    "        rpn_mode=rpn_mode,\n",
    "        normalization=(global_min, global_max)\n",
    "    )\n",
    "\n",
    "    train_set = VALDODataset(\n",
    "        cases=data_train.mri.tolist(),\n",
    "        masks=data_train.masks.tolist(),\n",
    "        target=data_train.target.tolist(),\n",
    "        transform=transform\n",
    "    )\n",
    "    val_set = VALDODataset(\n",
    "        cases=data_test.mri.tolist(),\n",
    "        masks=data_test.masks.tolist(),\n",
    "        target=data_test.target.tolist(),\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collatev2\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collatev2\n",
    "    )\n",
    "\n",
    "    if logger != None:\n",
    "        logger.info(s)\n",
    "    else:\n",
    "        print(s)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project import Fitter\n",
    "\n",
    "class ViTFitter(Fitter):\n",
    "    \n",
    "    def fit(self, train_loader, val_loader, stage1):\n",
    "        train_history = []\n",
    "        val_history = []\n",
    "        for epoch in range(self.epochs):\n",
    "            self.log(f'EPOCH {epoch} ==============================')\n",
    "            train_loss = self.train_one_epoch(train_loader, stage1)\n",
    "            val_loss = self.validation(val_loader, stage1)\n",
    "            train_history.append(train_loss)\n",
    "            val_history.append(val_loss)\n",
    "        return train_history, val_history\n",
    "\n",
    "    def train_one_epoch(self, train_loader, stage1):\n",
    "        self.model.train()\n",
    "        loss_history = []\n",
    "        counter = 0\n",
    "        for batch in train_loader:\n",
    "            Y = []\n",
    "            T = []\n",
    "            for slices, masks, target, case in batch:\n",
    "                slices = slices.squeeze(1).float()\n",
    "                masks = masks.float()\n",
    "\n",
    "                with torch.inference_mode():\n",
    "                    x, t = stage1(slices, masks, target)\n",
    "                \n",
    "                # self.log(f'{x.requires_grad}, {t.requires_grad}')\n",
    "                # self.log(f'{x.shape}, {t.shape}')\n",
    "\n",
    "                x = x.flatten(2).float().to(self.device)\n",
    "                t = t.flatten(2).float().to(self.device)\n",
    "                # self.log(f'XT SHAPES: {x.shape}, {t.shape}')\n",
    "                \n",
    "                y = self.model(x, target)\n",
    "                Y.append(y)\n",
    "                T.append(t[target])\n",
    "            \n",
    "            losses = self.loss(torch.stack(Y), torch.stack(T))\n",
    "            self.optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            self.optimizer.step()\n",
    "            counter += 1\n",
    "            self.log(f'Batch:\\t{counter}/{len(train_loader)}')\n",
    "            self.log(f'Batch samples:\\t{len(batch)}')\n",
    "            self.log(f'Current error:\\t{losses}\\n')\n",
    "            \n",
    "            loss_history.append(losses.detach().cpu().numpy())\n",
    "        \n",
    "        return loss_history\n",
    "    def validation(self, val_loader, stage1):\n",
    "        self.model.eval()\n",
    "        loss_history = []\n",
    "        with torch.inference_mode():\n",
    "            for batch in val_loader:\n",
    "                Y = []\n",
    "                T = []\n",
    "                for slices, masks, target, case in batch:\n",
    "                    slices = slices.squeeze(1).float()\n",
    "                    masks = masks.float()\n",
    "                    x, t = stage1(slices, masks, target)\n",
    "                    x = x.flatten(2).float().to(self.device)\n",
    "                    t = t.flatten(2).float().to(self.device)\n",
    "                    y = self.model(x, target)\n",
    "                    Y.append(y)\n",
    "                    T.append(t[target])\n",
    "                \n",
    "                losses = self.loss(torch.stack(Y), torch.stack(T))\n",
    "                loss_history.append(losses.cpu().numpy())\n",
    "        return loss_history"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
