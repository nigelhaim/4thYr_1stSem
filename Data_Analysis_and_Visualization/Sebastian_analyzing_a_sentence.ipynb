{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing a Sentence\n",
    "\n",
    "Lets start small. What can we learn about a sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\nigel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Preamble\n",
    "#\n",
    "\n",
    "# Import our core libraries\n",
    "import nltk\n",
    "import pprint\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "# Read in the documents we will use\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee = reuters.open('test/19570').read()\n",
    "gold = reuters.open('test/16589').read()\n",
    "\n",
    "text = gold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation and Tokenization\n",
    "\n",
    "The first step for us to do is to actually get a sentence from the document we loaded. The general process of extracting _meaningful units_ of text from a larger text is called __segmentation__.\n",
    "\n",
    "When applied to finding smaller word-like units from text, the process is often referred to as __tokenization__ (and you may find these terms used interchangeably). \n",
    "        \n",
    "A _meaningful unit_ is anything that is useful for your application, these can be _sections_ of a document, _paragraphs_, _words_ or in our case _sentences_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"HOMESTAKE &lt;HM> MULLS BUYING ORE RESERVES\\n  Homestake Mining Co is considering\\n  acquiring more gold ore reserves in addition to the company's\\n  exploration efforts, chief executive Harry Conger told Reuters\\n  in an interview\",\n",
       " '\\n      \"We are looking at more options to acquire more reserves\\n  rather than just exploration,\" Conger said adding, \"the move to\\n  consider acquisitions represents a change in the company\\'s\\n  acquisitions policy',\n",
       " '\"\\n      Conger said all of Homestake\\'s current cash position of 120\\n  mln dlrs would be available to acquire reserves',\n",
       " ' In addition,\\n  Homestake has two lines of credit totaling 150 mln dlrs which\\n  have not been drawn on today and could be used to finance an\\n  acquisition, he said',\n",
       " '\\n      Conger said he anticipates 1987 exploration budget will be\\n  about the same as 1986 spending of 27',\n",
       " '3 mln dlrs',\n",
       " \" Conger said\\n  exploration for precious metals may be slightly higher than\\n  last year's spending of 17\",\n",
       " \"7 mln dlrs while oil and gas\\n  exploration spending will be slightly less than last year's 9\",\n",
       " '6\\n  pct',\n",
       " \"\\n      Conger said he sees Homestake's 1987 gold production about\\n  the same as 1986 gold production of 669,594 ounces\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple sentence segmentation\n",
    "\n",
    "#\n",
    "# Using string.split\n",
    "#\n",
    "sentences = text.split('.')\n",
    "sentences[0:10]\n",
    "\n",
    "# Q1: Are there any issues with this approach?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"HOMESTAKE &lt;HM> MULLS BUYING ORE RESERVES\\n  Homestake Mining Co is considering\\n  acquiring more gold ore reserves in addition to the company's\\n  exploration efforts, chief executive Harry Conger told Reuters\\n  in an interview\",\n",
       " '\\n      \"We are looking at more options to acquire more reserves\\n  rather than just exploration,\" Conger said adding, \"the move to\\n  consider acquisitions represents a change in the company\\'s\\n  acquisitions policy',\n",
       " '\"\\n      Conger said all of Homestake\\'s current cash position of 120\\n  mln dlrs would be available to acquire reserves',\n",
       " ' In addition,\\n  Homestake has two lines of credit totaling 150 mln dlrs which\\n  have not been drawn on today and could be used to finance an\\n  acquisition, he said',\n",
       " '\\n      Conger said he anticipates 1987 exploration budget will be\\n  about the same as 1986 spending of 27',\n",
       " '3 mln dlrs',\n",
       " \" Conger said\\n  exploration for precious metals may be slightly higher than\\n  last year's spending of 17\",\n",
       " \"7 mln dlrs while oil and gas\\n  exploration spending will be slightly less than last year's 9\",\n",
       " '6\\n  pct',\n",
       " \"\\n      Conger said he sees Homestake's 1987 gold production about\\n  the same as 1986 gold production of 669,594 ounces\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Using re.split\n",
    "#\n",
    "sentences = re.split(r'[\\.\\?!]', text)\n",
    "sentences[0:10]\n",
    "\n",
    "# Q2: How about now? What other corner cases and caveats might \n",
    "#    we want to stay aware of?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nigel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"HOMESTAKE &lt;HM> MULLS BUYING ORE RESERVES\\n  Homestake Mining Co is considering\\n  acquiring more gold ore reserves in addition to the company's\\n  exploration efforts, chief executive Harry Conger told Reuters\\n  in an interview.\",\n",
       " '\"We are looking at more options to acquire more reserves\\n  rather than just exploration,\" Conger said adding, \"the move to\\n  consider acquisitions represents a change in the company\\'s\\n  acquisitions policy.\"',\n",
       " \"Conger said all of Homestake's current cash position of 120\\n  mln dlrs would be available to acquire reserves.\",\n",
       " 'In addition,\\n  Homestake has two lines of credit totaling 150 mln dlrs which\\n  have not been drawn on today and could be used to finance an\\n  acquisition, he said.',\n",
       " 'Conger said he anticipates 1987 exploration budget will be\\n  about the same as 1986 spending of 27.3 mln dlrs.',\n",
       " \"Conger said\\n  exploration for precious metals may be slightly higher than\\n  last year's spending of 17.7 mln dlrs while oil and gas\\n  exploration spending will be slightly less than last year's 9.6\\n  pct.\",\n",
       " \"Conger said he sees Homestake's 1987 gold production about\\n  the same as 1986 gold production of 669,594 ounces.\",\n",
       " \"However, 1987 first quarter production from its McLaughlin\\n  reserve will be about 10 pct lower than last year's 45,400\\n  ounces due to start-up production problems.\",\n",
       " 'He said he believes gold prices will hold above the 400\\n  U.S. dlr an ounce level for the rest of 1987.',\n",
       " 'IN 1986, company earnings were based an average market\\n  price for gold of 368 dlrs an ounce.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "# Using NLTK.\n",
    "# \n",
    "nltk.download('punkt_tab')\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "sentences[0:10]\n",
    "\n",
    "# Notice that this retains the punctuation in the sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox jumped over Mr. Jones.',\n",
       " 'Much to the disapproval of the dogs.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK will also handle cases like Mr. Jones correctly.\n",
    "with_salutations = \"\"\"The quick brown fox jumped over Mr. Jones.\n",
    "                      Much to the disapproval of the dogs.\"\"\"\n",
    "nltk.sent_tokenize(with_salutations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization\n",
    "\n",
    "We can apply the same approach to finding word-like units of text. All the above methods could be used but similar caveats will apply. So lets jump straight into using nltk for the tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The : 3\n",
      "quick : 5\n",
      "brown : 5\n",
      "fox : 3\n",
      "jumped : 6\n",
      "over : 4\n",
      "Mr. : 3\n",
      "Jones : 5\n",
      ". : 1\n",
      "Much : 4\n",
      "to : 2\n",
      "the : 3\n",
      "disapproval : 11\n",
      "of : 2\n",
      "the : 3\n",
      "dogs : 4\n",
      ". : 1\n"
     ]
    }
   ],
   "source": [
    "# We first need to tokenize our sentence into smaller units, in our case words.\n",
    "with_salutations = \"\"\"The quick brown fox jumped over Mr. Jones.\n",
    "                      Much to the disapproval of the dogs.\"\"\"\n",
    "words = nltk.word_tokenize(with_salutations)\n",
    "\n",
    "# Lets print out all the things in this list and the lengths of the words\n",
    "# Notice that python is whitespace sensitive with indentation\n",
    "# indicating block structure.\n",
    "for word in words:\n",
    "    word_len = len(word)\n",
    "    print(word + ' : ' + str(word_len))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Your Turn!__\n",
    "\n",
    "Plot the lengths of all the sentences in the __gold__ article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "# Plot the lengths of all the sentences in the __gold__ article.\n",
    "# First print them and then see if you can make an ascii bar chart of them.\n",
    "# It could look something like this...\n",
    "#\n",
    "# ****\n",
    "# ******************\n",
    "# ***********\n",
    "#\n",
    "\n",
    "gold = reuters.open('test/16589').read()\n",
    "\n",
    "# Step one. Find all the lengths and print them to the console/notebook.\n",
    "\n",
    "# Step two (extra credit). Make an horizontal ascii bar graph of the numbers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228\n",
      "207\n",
      "110\n",
      "163\n",
      "110\n",
      "204\n",
      "112\n",
      "165\n",
      "103\n",
      "93\n",
      "230\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(gold)\n",
    "sentences_lengths = [len(sentence) for sentence in sentences]\n",
    "\n",
    "for length in sentences_lengths:\n",
    "    print(length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228 ************************************************************************************************************************************************************************************************************************************\n",
      "207 ***************************************************************************************************************************************************************************************************************\n",
      "110 **************************************************************************************************************\n",
      "163 *******************************************************************************************************************************************************************\n",
      "110 **************************************************************************************************************\n",
      "204 ************************************************************************************************************************************************************************************************************\n",
      "112 ****************************************************************************************************************\n",
      "165 *********************************************************************************************************************************************************************\n",
      "103 *******************************************************************************************************\n",
      " 93 *********************************************************************************************\n",
      "230 **************************************************************************************************************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "#Ascii bargraph\n",
    "for length in sentences_lengths:\n",
    "    if(length <= 99):\n",
    "        print(\"0\\b\",length,('*' * length))\n",
    "    else:\n",
    "        print(length,('*' * length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech\n",
    "\n",
    "What else can we find out about this sentence. We can look for specific patterns we have already identified, but we can also try and deduce what type of this word this is, e.g. is it a noun or a verb. \n",
    "\n",
    "This activity is known as __Part of Speech Tagging__ _(POS Tagging)_, and a number of algorithms have been developed to do this, some are statistical and others are rule based. \n",
    "\n",
    "Lets look at how we can do this in nltk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOMESTAKE &lt;HM> MULLS BUYING ORE RESERVES\n",
      "  Homestake Mining Co is considering\n",
      "  acquiring more gold ore reserves in addition to the company's\n",
      "  exploration efforts, chief executive Harry Conger told Reuters\n",
      "  in an interview.\n"
     ]
    }
   ],
   "source": [
    "# Lets look at the first sentence of the article\n",
    "sentence = sentences[0]\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HOMESTAKE',\n",
       " '&',\n",
       " 'lt',\n",
       " ';',\n",
       " 'HM',\n",
       " '>',\n",
       " 'MULLS',\n",
       " 'BUYING',\n",
       " 'ORE',\n",
       " 'RESERVES',\n",
       " 'Homestake',\n",
       " 'Mining',\n",
       " 'Co',\n",
       " 'is',\n",
       " 'considering',\n",
       " 'acquiring',\n",
       " 'more',\n",
       " 'gold',\n",
       " 'ore',\n",
       " 'reserves',\n",
       " 'in',\n",
       " 'addition',\n",
       " 'to',\n",
       " 'the',\n",
       " 'company',\n",
       " \"'s\",\n",
       " 'exploration',\n",
       " 'efforts',\n",
       " ',',\n",
       " 'chief',\n",
       " 'executive',\n",
       " 'Harry',\n",
       " 'Conger',\n",
       " 'told',\n",
       " 'Reuters',\n",
       " 'in',\n",
       " 'an',\n",
       " 'interview',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We first need to tokenize our sentence into smaller units, in our case words.\n",
    "words = nltk.word_tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note that the result includes punctuation as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\nigel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('HOMESTAKE', 'NNP'),\n",
       " ('&', 'CC'),\n",
       " ('lt', 'NN'),\n",
       " (';', ':'),\n",
       " ('HM', 'NNP'),\n",
       " ('>', 'NNP'),\n",
       " ('MULLS', 'NNP'),\n",
       " ('BUYING', 'NNP'),\n",
       " ('ORE', 'NNP'),\n",
       " ('RESERVES', 'NNP'),\n",
       " ('Homestake', 'NNP'),\n",
       " ('Mining', 'NNP'),\n",
       " ('Co', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('considering', 'VBG'),\n",
       " ('acquiring', 'VBG'),\n",
       " ('more', 'JJR'),\n",
       " ('gold', 'NN'),\n",
       " ('ore', 'NN'),\n",
       " ('reserves', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('addition', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('company', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('exploration', 'NN'),\n",
       " ('efforts', 'NNS'),\n",
       " (',', ','),\n",
       " ('chief', 'JJ'),\n",
       " ('executive', 'NN'),\n",
       " ('Harry', 'NNP'),\n",
       " ('Conger', 'NNP'),\n",
       " ('told', 'VBD'),\n",
       " ('Reuters', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('interview', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the words we can use nltk's POS tagger\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "tagged = nltk.pos_tag(words)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tags like NN and NNP come from a standardized set of tags known as the __Penn Treebank POS Tags__\n",
    "\n",
    "You can see the full list here https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets extract all the nouns from all the sentences in the document\n",
    "\n",
    "# Will return all the words in the words input param that match a given tag.\n",
    "def get_tagged(words, tag):\n",
    "    tagged = nltk.pos_tag(words)\n",
    "\n",
    "    matches = []\n",
    "    for tup in tagged:\n",
    "        if tup[1] == tag:\n",
    "            matches.append(tup)\n",
    "    return matches\n",
    "\n",
    "# This is the same function as above, but uses a List Comprehension\n",
    "def get_tagged(words, tag):\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    return [tup for tup in tagged if tup[1] == tag]\n",
    "\n",
    "# Will word tokenize a list of sentence and return those tokens\n",
    "def tokenize(sentences):\n",
    "    return [nltk.word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized = tokenize(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('HOMESTAKE', 'NNP'),\n",
       "  ('HM', 'NNP'),\n",
       "  ('>', 'NNP'),\n",
       "  ('MULLS', 'NNP'),\n",
       "  ('BUYING', 'NNP'),\n",
       "  ('ORE', 'NNP'),\n",
       "  ('RESERVES', 'NNP'),\n",
       "  ('Homestake', 'NNP'),\n",
       "  ('Mining', 'NNP'),\n",
       "  ('Co', 'NNP'),\n",
       "  ('Harry', 'NNP'),\n",
       "  ('Conger', 'NNP'),\n",
       "  ('Reuters', 'NNP')],\n",
       " [('Conger', 'NNP')],\n",
       " [('Conger', 'NNP'), ('Homestake', 'NNP')],\n",
       " [('Homestake', 'NNP')],\n",
       " [('Conger', 'NNP')],\n",
       " [('Conger', 'NNP')],\n",
       " [('Conger', 'NNP'), ('Homestake', 'NNP')],\n",
       " [('McLaughlin', 'NNP')],\n",
       " [('U.S.', 'NNP')],\n",
       " [],\n",
       " [('Conger', 'NNP')]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnps = [get_tagged(tokens, 'NNP') for tokens in tokenized]\n",
    "nnps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see. POS Tagging takes a little bit of time. It can also yield imperfect results. However it can be an interesting approach to finding out more about a text.\n",
    "\n",
    "It is also useful to keep in mind that there are actually a number of different algorithms and models that can be used for POS tagging. \n",
    "\n",
    "See http://www.nltk.org/book/ch05.html and http://www.nltk.org/api/nltk.tag.html for a reference to what is in NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Your Turn!__\n",
    "\n",
    "Plot the number of verbs (VB, VBD, VBG, VBN, VBP) across all sentences in the gold article. Which sentence has the most number of verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exercise 2\n",
    "# Count the number of verbs (VB) in each sentence of the gold article\n",
    "# You may want to make some helper functions.\n",
    "\n",
    "\n",
    "gold = reuters.open('test/16589').read()\n",
    "\n",
    "\n",
    "# Step 1. Tokenize the article to sentences\n",
    "\n",
    "# Step 2. For each sentence tokenize to words and store that list\n",
    "\n",
    "# Step 3. POS tag all the tokens in each sentence and filter out non verb \n",
    "#         tokens.\n",
    "\n",
    "# Step 4. Print out the counts for each sentence. Which sentence\n",
    "#         has the most verbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  8 Verbs:  [[('acquire', 'VB'), ('consider', 'VB')], [('be', 'VB'), ('acquire', 'VB')], [('be', 'VB'), ('finance', 'VB')], [('be', 'VB')], [('be', 'VB'), ('be', 'VB')], [('be', 'VB')], [('hold', 'VB')], [('give', 'VB'), ('be', 'VB')]]\n",
      "Length:  10 Verbs:  [[('told', 'VBD')], [('said', 'VBD')], [('said', 'VBD')], [('said', 'VBD')], [('said', 'VBD')], [('said', 'VBD')], [('said', 'VBD')], [('said', 'VBD'), ('dlr', 'VBD')], [('were', 'VBD')], [('said', 'VBD'), ('declined', 'VBD')]]\n",
      "Length:  6 Verbs:  [[('considering', 'VBG'), ('acquiring', 'VBG')], [('looking', 'VBG'), ('adding', 'VBG')], [('totaling', 'VBG')], [], [], []]\n",
      "Length:  6 Verbs:  [[('been', 'VBN'), ('drawn', 'VBN'), ('used', 'VBN')], [], [], [], [('based', 'VBN')], [('released', 'VBN')]]\n",
      "Length:  5 Verbs:  [[('are', 'VBP')], [('have', 'VBP')], [], [], []]\n",
      " 8 ********\n",
      " 10 **********\n",
      " 6 ******\n",
      " 6 ******\n",
      " 5 *****\n"
     ]
    }
   ],
   "source": [
    "#tokenize the article to sentences\n",
    "sentences = nltk.sent_tokenize(gold)\n",
    "#tokenize each sentence to words\n",
    "tokenized = tokenize(sentences)\n",
    "#POS tag all the tokens in each sentence and filter out non verb tokens\n",
    "verbs = []\n",
    "\n",
    "\n",
    "verbs.append([get_tagged(tokens, 'VB')for tokens in tokenized])\n",
    "verbs.append([get_tagged(tokens, 'VBD')for tokens in tokenized])\n",
    "verbs.append([get_tagged(tokens, 'VBG')for tokens in tokenized])\n",
    "verbs.append([get_tagged(tokens, 'VBN')for tokens in tokenized])\n",
    "verbs.append([get_tagged(tokens, 'VBP')for tokens in tokenized])\n",
    "\n",
    "\n",
    "#Remove empty lists\n",
    "for vl in verbs:\n",
    "    for v in vl:\n",
    "        if len(v) == 0 or v == []:\n",
    "            vl.remove(v)\n",
    "#Print out the counts for each sentence\n",
    "for verb in verbs:\n",
    "    print(\"Length: \", len(verb), \"Verbs: \", verb)\n",
    "\n",
    "# Generate asciii bar graph\n",
    "for verb in verbs:\n",
    "    if(len(verb) <= 99):\n",
    "        print(\"0\\b\",len(verb),('*' * len(verb)))\n",
    "    else:\n",
    "        print(len(verb),('*' * len(verb)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "One way we can think of what we have been just been doing is finding _'things'_ that give some information about our sentences. Features cound be anything countable, whether it is is the amount of money mentioned in a sentence, or the number of characters. A big part of text analysis, particularly the statistical and pattern based approaches we will be looking at is feature selection and extraction.\n",
    "\n",
    "Q3: What other features could you think to extract from a sentence?\n",
    "\n",
    "\n",
    "# Examples\n",
    "\n",
    "We can also see how even simple seeming features can be used to good effect in building visualizations.\n",
    "\n",
    "[Listening Post](https://vimeo.com/3885443) by Mark Hansen and Ben Rubin is an installation that displays sentence gathered from the internet that contain phrases like \"I am\" or \"I love.\"\n",
    "\n",
    "[We Feel Fine](http://wefeelfine.org/) by Jonathan Harris and Sep Kamvar begins by searching for blog posts that containt phrases like \"I am feeling\", \"I feel\" ... [Regex]\n",
    "\n",
    "[Stereotropes](http://stereotropes.bocoup.com/) by Bocoup uses POS tagging to extract adjectives from text to then visualize descriptions of characters in film. [POS Tagging]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
