{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing a Document\n",
    "\n",
    "We are going to move from an individual sentence to a document. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "This notebook expects that the needed packages have already been installed and that NLTK has been setup correctly via the `setup` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Text\n",
    "\n",
    "First, lets get some text data. \n",
    "\n",
    "Here we use what we've learned about python to read in some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144342"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"Datasets/alice.txt\"\n",
    "\n",
    "with open(filename) as handle:\n",
    "    text = handle.read()\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHAPTER I. Down the Rabbit-Hole\\n\\nAlice was beginni'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real quick, lets take away newline characters to be able to read the text better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHAPTER I. Down the Rabbit-Hole  Alice was beginni'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove new line characters\n",
    "text = re.sub(r'\\n', ' ', text)\n",
    "\n",
    "text[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "From our Sentence analysis, we know that the process of splitting up a document into small, word-like _meaningful units_ is known as **tokenization**. \n",
    "\n",
    "We will start with word tokenization and look at multi-word tokens in a bit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33533"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use word tokenizer\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAPTER',\n",
       " 'I',\n",
       " '.',\n",
       " 'Down',\n",
       " 'the',\n",
       " 'Rabbit-Hole',\n",
       " 'Alice',\n",
       " 'was',\n",
       " 'beginning',\n",
       " 'to',\n",
       " 'get',\n",
       " 'very',\n",
       " 'tired',\n",
       " 'of',\n",
       " 'sitting',\n",
       " 'by',\n",
       " 'her',\n",
       " 'sister',\n",
       " 'on',\n",
       " 'the']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, NLTK's word tokenizer leaves punctuation as separate tokens. We will take care of that in a second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Searching in a Document\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has some search features in their `Text` object. \n",
    "\n",
    "Useful for interactive searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAPTER',\n",
       " 'I',\n",
       " '.',\n",
       " 'Down',\n",
       " 'the',\n",
       " 'Rabbit-Hole',\n",
       " 'Alice',\n",
       " 'was',\n",
       " 'beginning',\n",
       " 'to',\n",
       " 'get',\n",
       " 'very',\n",
       " 'tired',\n",
       " 'of',\n",
       " 'sitting',\n",
       " 'by',\n",
       " 'her',\n",
       " 'sister',\n",
       " 'on',\n",
       " 'the']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.Text has useful methods for searching\n",
    "my_text = nltk.Text(tokens)\n",
    "\n",
    "# But it initially looks the same as are tokens array\n",
    "my_text[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`findall`** takes a string with tokens delimited with angle brackets `<token>`. Regular expressions can be used in and around the angle brackets to robustly find token matches.\n",
    "\n",
    "Here we find the preceding and following word for every use of \"Hare\" in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "March Hare .; March Hare was; March Hare will; March Hare :; March\n",
      "Hare and; March Hare said; March Hare .; March Hare .; March Hare .;\n",
      "March Hare went; March Hare ,; March Hare .; March Hare meekly; March\n",
      "Hare took; March Hare .; March Hare said; March Hare ,; March Hare\n",
      "interrupted; March Hare .; March Hare said; March Hare went; March\n",
      "Hare moved; March Hare .; March Hare had; March Hare .; March Hare ,;\n",
      "March Hare .; March Hare said; March Hare interrupted; March Hare .;\n",
      "March Hare and\n"
     ]
    }
   ],
   "source": [
    "my_text.findall('<.*><Hare><.*>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`concordance`** searches for a word in a document and displays matches in their original contexts. \n",
    "\n",
    "\n",
    "Let's create a KWIC for lines that include the word _Hare_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 31 matches:\n",
      "aving the other paw , 'lives a March Hare . Visit either you like : they 're b\n",
      " in the direction in which the March Hare was said to live . ' I 've seen hatt\n",
      ", ' she said to herself ; 'the March Hare will be much the most interesting , \n",
      "e in sight of the house of the March Hare : she thought it must be the right h\n",
      "n front of the house , and the March Hare and the Hatter were having tea at it\n",
      "able . 'Have some wine , ' the March Hare said in an encouraging tone . Alice \n",
      "'There is n't any , ' said the March Hare . 'Then it was n't very civil of you\n",
      "out being invited , ' said the March Hare . ' I did n't know it was YOUR table\n",
      " the answer to it ? ' said the March Hare . 'Exactly so , ' said Alice . 'Then\n",
      "ould say what you mean , ' the March Hare went on . ' I do , ' Alice hastily r\n",
      "just as well say , ' added the March Hare , 'that `` I like what I get '' is t\n",
      "e added looking angrily at the March Hare . 'It was the BEST butter , ' the Ma\n",
      "It was the BEST butter , ' the March Hare meekly replied . 'Yes , but some cru\n",
      "n with the bread-knife . ' The March Hare took the watch and looked at it gloo\n",
      "e Hatter . 'Nor I , ' said the March Hare . Alice sighed wearily . ' I think y\n",
      "( ' I only wish it was , ' the March Hare said to itself in a whisper . ) 'Tha\n",
      "ting with his tea spoon at the March Hare , ) ' -- it was at the great concert\n",
      " we change the subject , ' the March Hare interrupted , yawning . ' I 'm getti\n",
      " 'Tell us a story ! ' said the March Hare . 'Yes , please do ! ' pleaded Alice\n",
      " ' 'Take some more tea , ' the March Hare said to Alice , very earnestly . ' I\n",
      "grily , but the Hatter and the March Hare went 'Sh ! sh ! ' and the Dormouse s\n",
      "he Dormouse followed him : the March Hare moved into the Dormouse 's place , a\n",
      "illingly took the place of the March Hare . The Hatter was the only one who go\n",
      "worse off than before , as the March Hare had just upset the milk-jug into his\n",
      " Alice . 'Why not ? ' said the March Hare . Alice was silent . The Dormouse ha\n"
     ]
    }
   ],
   "source": [
    "my_text.concordance('Hare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **concordance** is a listing of all the words in a book or other document. Typically the presentation of concordance lines used here is known as a [keyword-in-context](https://en.wikipedia.org/wiki/Key_Word_in_Context) (KWIC) visual.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red\" class=\"exercise\">Your Turn</h3>\n",
    "\n",
    "Let's do some more searching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your code:\n",
    "# Search for other characters in the text. Where does Hatter show up? What about Alice? \n",
    "# Can you use your Regex knowledge to find uppercase and lowercase 'hatter' references?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a Hatter :; the Hatter instead; the Hatter were; the Hatter .; The\n",
      "Hatter opened; the Hatter .; the Hatter ,; The Hatter was; the Hatter\n",
      ".; the Hatter grumbled; the Hatter .; the Hatter .; The Hatter 's; the\n",
      "Hatter ,; the Hatter said; the Hatter .; the Hatter ,; the Hatter\n",
      "said; the Hatter .; the Hatter :; The Hatter shook; the Hatter\n",
      "continued; the Hatter ,; the Hatter went; the Hatter with; the Hatter\n",
      ":; the Hatter ,; the Hatter :; the Hatter asked; the Hatter and; the\n",
      "Hatter :; The Hatter was; the Hatter ;; the Hatter ,; the Hatter .;\n",
      "the Hatter .; The Hatter looked; the Hatter .; the Hatter .; the\n",
      "Hatter added; the Hatter ,; the Hatter ,; wretched Hatter trembled;\n",
      "the Hatter began; the Hatter replied; the Hatter went; the Hatter .;\n",
      "the Hatter went; the Hatter ,; the Hatter .; miserable Hatter dropped;\n",
      "the Hatter :; the Hatter ,; the Hatter hurriedly; the Hatter was\n"
     ]
    }
   ],
   "source": [
    "# Search for other characters in the text. Where does Hatter show up?\n",
    "my_text.findall('<.*><Hatter><.*>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rabbit-Hole Alice was; thought Alice 'without; did Alice think; ,\n",
      "Alice started; went Alice after; that Alice had; thought Alice to; ,\n",
      "Alice had; ( Alice had; so Alice soon; here Alice began; . Alice was;\n",
      "went Alice like; when Alice had; and Alice 's; ! Alice opened; poor\n",
      "Alice ,; that Alice had; said Alice ,; little Alice was; so Alice\n",
      "ventured; said Alice ;; said Alice to; poor Alice !; said Alice to;\n",
      "poor Alice ,; said Alice ,; but Alice had; cried Alice (; thought\n",
      "Alice ,; Poor Alice !; said Alice ,; ' Alice felt; . Alice took; poor\n",
      "Alice ,; cried Alice ,; said Alice ,; ( Alice had; said Alice ,;\n",
      "thought Alice ,; ( Alice thought; thought Alice ;; , Alice had; cried\n",
      "Alice hastily; said Alice in; ' Alice went; cried Alice again; said\n",
      "Alice ,; so Alice went; cried Alice in; , Alice thought; . Alice led;\n",
      "to Alice to; this Alice would; . Alice kept; to Alice as; said Alice\n",
      "in; said Alice ;; to Alice with; ' Alice had; to Alice .; said Alice\n",
      "sadly; . Alice thought; said Alice ,; to Alice ,; said Alice ,; to\n",
      "Alice severely; said Alice very; said Alice ,; poor Alice .; ' Alice\n",
      "called; said Alice aloud; . Alice replied; and Alice was; poor Alice\n",
      "began; ' Alice guessed; noticed Alice ,; And Alice was; ' Alice said;\n",
      "Miss Alice !; ' Alice went; for Alice ,; poor Alice ,; thought Alice\n",
      ",; foolish Alice !; . Alice knew; and Alice 's; . Alice heard; thought\n",
      "Alice ,; and Alice could; thought Alice .; said Alice to; thought\n",
      "Alice ,; and Alice called; and Alice thought; and Alice heard; thought\n",
      "Alice ;; . Alice noticed; at Alice the; said Alice to; said Alice ,;\n",
      "then Alice dodged; then Alice ,; to Alice a; said Alice ,; ? Alice\n",
      "looked; and Alice looked; . Alice replied; said Alice ,; ' Alice\n",
      "replied; said Alice ;; said Alice ;; . Alice felt; as Alice could; :\n",
      "Alice turned; said Alice ,; . Alice thought; said Alice ;; ' Alice\n",
      "replied; . Alice folded; said Alice ,; ' Alice hastily; . Alice said;\n",
      "said Alice :; poor Alice in; time Alice waited; thought Alice to; .\n",
      "Alice remained; said Alice in; said Alice .; said Alice indignantly;\n",
      "said Alice .; ' Alice was; said Alice ,; said Alice .; said Alice ,;\n",
      "said Alice ,; to Alice ,; said Alice hastily; . Alice crouched;\n",
      "thought Alice ,; , Alice noticed; . Alice laughed; . Alice went; said\n",
      "Alice ,; this Alice thought; asked Alice again; only Alice did; said\n",
      "Alice .; said Alice desperately; ' Alice said; said Alice ,; that\n",
      "Alice quite; ' Alice said; ' Alice did; cried Alice ,; said Alice ,; '\n",
      "Alice glanced; that Alice could; to Alice ,; . Alice caught; thought\n",
      "Alice .; thought Alice ,; said Alice ;; and Alice looked; altogether\n",
      "Alice did; said Alice ,; . Alice was; saw Alice .; thought Alice ,;\n",
      "said Alice .; ' Alice added; ' Alice felt; ' Alice remarked; said\n",
      "Alice .; ' Alice did; said Alice .; said Alice .; said Alice ,; .\n",
      "Alice was; ' Alice quietly; . Alice waited; replied Alice ;; thought\n",
      "Alice ;; thought Alice ;; saw Alice coming; said Alice indignantly; .\n",
      "Alice looked; said Alice angrily; said Alice ;; at Alice for; ' Alice\n",
      "said; thought Alice .; said Alice .; ' Alice hastily; while Alice\n",
      "thought; to Alice :; . Alice considered; ' Alice had; ' Alice replied;\n",
      ". Alice felt; to Alice again; ' Alice replied; . Alice sighed; said\n",
      "Alice .; ' Alice cautiously; said Alice thoughtfully; ' Alice asked;\n",
      "said Alice .; exclaimed Alice .; into Alice 's; said Alice .; ' Alice\n",
      "ventured; said Alice ,; pleaded Alice .; said Alice ,; ' Alice gently;\n",
      "' Alice tried; to Alice ,; ' Alice replied; said Alice .; . Alice did;\n",
      "' Alice was; ' Alice said; said Alice ,; and Alice rather; and Alice\n",
      "was; . Alice did; ' Alice said; poor Alice ,; said Alice .; . Alice\n",
      "was; said Alice ,; than Alice could; said Alice as; . Alice thought;\n",
      "upon Alice ,; said Alice ,; and Alice looked; them Alice recognised; .\n",
      "Alice was; to Alice ,; to Alice ,; is Alice ,; said Alice very; said\n",
      "Alice ,; said Alice ,; to Alice for; said Alice ,; at Alice ,; shouted\n",
      "Alice .; and Alice joined; said Alice :; said Alice .; said Alice :; .\n",
      "Alice gave; . Alice thought; difficulty Alice found; , Alice soon; .\n",
      "Alice began; . Alice waited; then Alice put; ' Alice began; said Alice\n",
      ":; to Alice ,; said Alice :; behind Alice as; said Alice .; . Alice\n",
      "thought; to Alice an; where Alice could; thought Alice ,; moment Alice\n",
      "appeared; ) Alice could; into Alice 's; . Alice was; ' Alice ventured;\n",
      "to Alice 's; . Alice did; upon Alice 's; ' Alice whispered; into Alice\n",
      "'s; ' Alice thought; ' Alice cautiously; ' Alice remarked; said Alice\n",
      ".; that Alice said; exclaimed Alice ,; ' Alice said; said Alice .;\n",
      "thought Alice .; said Alice sharply; to Alice 's; . Alice looked; to\n",
      "Alice ;; and Alice was; and Alice ,; to Alice ,; said Alice .; said\n",
      "Alice .; , Alice heard; leaving Alice alone; . Alice did; to Alice .;\n",
      "said Alice .; thought Alice ,; , Alice could; . Alice thought; . Alice\n",
      "was; ' Alice asked; poor Alice ,; interrupted Alice .; before Alice\n",
      "could; said Alice ;; said Alice ,; said Alice indignantly; said Alice\n",
      ";; inquired Alice .; ' Alice ventured; said Alice doubtfully; ' Alice\n",
      "did; said Alice .; said Alice ,; exclaimed Alice .; to Alice ,; '\n",
      "Alice went; at Alice ,; said Alice ); ( Alice began; said Alice .; at\n",
      "Alice .; said Alice timidly; said Alice .; round Alice ,; said Alice\n",
      ",; said Alice ,; ' Alice replied; said Alice ,; said Alice .; . Alice\n",
      "was; ' Alice looked; ' Alice asked; said Alice ,; said Alice in; said\n",
      "Alice .; said Alice a; So Alice began; over Alice .; thought Alice ;;\n",
      "' Alice said; ' Alice said; ' Alice did; and Alice was; ' Alice\n",
      "replied; taking Alice by; ' Alice panted; made Alice quite; . Alice\n",
      "had; thought Alice ,; ' Alice whispered; ' Alice began; . Alice could;\n",
      "thought Alice .; , Alice could; moment Alice felt; said Alice very;\n",
      "said Alice more; thought Alice .; thought Alice .; and Alice guessed;\n",
      "' Alice watched; . Alice 's; cried Alice ,; at Alice as; . Alice\n",
      "looked; to Alice .; said Alice .; said Alice .; ' Alice could; at\n",
      "Alice .; said Alice .; said Alice :; said Alice .; said Alice .; said\n",
      "Alice ,; said Alice .; said Alice loudly; said Alice .; said Alice ,;\n",
      ", Alice dear; said Alice ,; So Alice got; little Alice and; little\n",
      "Alice herself\n"
     ]
    }
   ],
   "source": [
    "# What about Alice?\n",
    "my_text.findall('<.*><Alice><.*>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a Hatter :; the Hatter instead; the Hatter were; the Hatter .; The\n",
      "Hatter opened; the Hatter .; the Hatter ,; The Hatter was; the Hatter\n",
      ".; the Hatter grumbled; the Hatter .; the Hatter .; The Hatter 's; the\n",
      "Hatter ,; the Hatter said; the Hatter .; the Hatter ,; the Hatter\n",
      "said; the Hatter .; the Hatter :; The Hatter shook; the Hatter\n",
      "continued; the Hatter ,; the Hatter went; the Hatter with; the Hatter\n",
      ":; the Hatter ,; the Hatter :; the Hatter asked; the Hatter and; the\n",
      "Hatter :; The Hatter was; the Hatter ;; the Hatter ,; the Hatter .;\n",
      "the Hatter .; The Hatter looked; the Hatter .; the Hatter .; the\n",
      "Hatter added; a hatter .; the Hatter ,; the Hatter ,; wretched Hatter\n",
      "trembled; the Hatter began; the Hatter replied; the Hatter went; the\n",
      "Hatter .; the Hatter went; the Hatter ,; the Hatter .; miserable\n",
      "Hatter dropped; the Hatter :; the Hatter ,; the Hatter hurriedly; the\n",
      "Hatter was\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can you use your Regex knowledge to find uppercase and lowercase 'hatter' references?\n",
    "regex = my_text.findall('<.*><[Hh]atter><.*>')\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rabbit-Hole Alice was; thought Alice 'without; did Alice think; ,\n",
      "Alice started; went Alice after; that Alice had; thought Alice to; ,\n",
      "Alice had; ( Alice had; so Alice soon; here Alice began; . Alice was;\n",
      "went Alice like; when Alice had; and Alice 's; ! Alice opened; poor\n",
      "Alice ,; that Alice had; said Alice ,; little Alice was; so Alice\n",
      "ventured; said Alice ;; said Alice to; poor Alice !; said Alice to;\n",
      "poor Alice ,; said Alice ,; but Alice had; cried Alice (; thought\n",
      "Alice ,; Poor Alice !; said Alice ,; ' Alice felt; . Alice took; poor\n",
      "Alice ,; cried Alice ,; said Alice ,; ( Alice had; said Alice ,;\n",
      "thought Alice ,; ( Alice thought; thought Alice ;; , Alice had; cried\n",
      "Alice hastily; said Alice in; ' Alice went; cried Alice again; said\n",
      "Alice ,; so Alice went; cried Alice in; , Alice thought; . Alice led;\n",
      "to Alice to; this Alice would; . Alice kept; to Alice as; said Alice\n",
      "in; said Alice ;; to Alice with; ' Alice had; to Alice .; said Alice\n",
      "sadly; . Alice thought; said Alice ,; to Alice ,; said Alice ,; to\n",
      "Alice severely; said Alice very; said Alice ,; poor Alice .; ' Alice\n",
      "called; said Alice aloud; . Alice replied; and Alice was; poor Alice\n",
      "began; ' Alice guessed; noticed Alice ,; And Alice was; ' Alice said;\n",
      "Miss Alice !; ' Alice went; for Alice ,; poor Alice ,; thought Alice\n",
      ",; foolish Alice !; . Alice knew; and Alice 's; . Alice heard; thought\n",
      "Alice ,; and Alice could; thought Alice .; said Alice to; thought\n",
      "Alice ,; and Alice called; and Alice thought; and Alice heard; thought\n",
      "Alice ;; . Alice noticed; at Alice the; said Alice to; said Alice ,;\n",
      "then Alice dodged; then Alice ,; to Alice a; said Alice ,; ? Alice\n",
      "looked; and Alice looked; . Alice replied; said Alice ,; ' Alice\n",
      "replied; said Alice ;; said Alice ;; . Alice felt; as Alice could; :\n",
      "Alice turned; said Alice ,; . Alice thought; said Alice ;; ' Alice\n",
      "replied; . Alice folded; said Alice ,; ' Alice hastily; . Alice said;\n",
      "said Alice :; poor Alice in; time Alice waited; thought Alice to; .\n",
      "Alice remained; said Alice in; said Alice .; said Alice indignantly;\n",
      "said Alice .; ' Alice was; said Alice ,; said Alice .; said Alice ,;\n",
      "said Alice ,; to Alice ,; said Alice hastily; . Alice crouched;\n",
      "thought Alice ,; , Alice noticed; . Alice laughed; . Alice went; said\n",
      "Alice ,; this Alice thought; asked Alice again; only Alice did; said\n",
      "Alice .; said Alice desperately; ' Alice said; said Alice ,; that\n",
      "Alice quite; ' Alice said; ' Alice did; cried Alice ,; said Alice ,; '\n",
      "Alice glanced; that Alice could; to Alice ,; . Alice caught; thought\n",
      "Alice .; thought Alice ,; said Alice ;; and Alice looked; altogether\n",
      "Alice did; said Alice ,; . Alice was; saw Alice .; thought Alice ,;\n",
      "said Alice .; ' Alice added; ' Alice felt; ' Alice remarked; said\n",
      "Alice .; ' Alice did; said Alice .; said Alice .; said Alice ,; .\n",
      "Alice was; ' Alice quietly; . Alice waited; replied Alice ;; thought\n",
      "Alice ;; thought Alice ;; saw Alice coming; said Alice indignantly; .\n",
      "Alice looked; said Alice angrily; said Alice ;; at Alice for; ' Alice\n",
      "said; thought Alice .; said Alice .; ' Alice hastily; while Alice\n",
      "thought; to Alice :; . Alice considered; ' Alice had; ' Alice replied;\n",
      ". Alice felt; to Alice again; ' Alice replied; . Alice sighed; said\n",
      "Alice .; ' Alice cautiously; said Alice thoughtfully; ' Alice asked;\n",
      "said Alice .; exclaimed Alice .; into Alice 's; said Alice .; ' Alice\n",
      "ventured; said Alice ,; pleaded Alice .; said Alice ,; ' Alice gently;\n",
      "' Alice tried; to Alice ,; ' Alice replied; said Alice .; . Alice did;\n",
      "' Alice was; ' Alice said; said Alice ,; and Alice rather; and Alice\n",
      "was; . Alice did; ' Alice said; poor Alice ,; said Alice .; . Alice\n",
      "was; said Alice ,; than Alice could; said Alice as; . Alice thought;\n",
      "upon Alice ,; said Alice ,; and Alice looked; them Alice recognised; .\n",
      "Alice was; to Alice ,; to Alice ,; is Alice ,; said Alice very; said\n",
      "Alice ,; said Alice ,; to Alice for; said Alice ,; at Alice ,; shouted\n",
      "Alice .; and Alice joined; said Alice :; said Alice .; said Alice :; .\n",
      "Alice gave; . Alice thought; difficulty Alice found; , Alice soon; .\n",
      "Alice began; . Alice waited; then Alice put; ' Alice began; said Alice\n",
      ":; to Alice ,; said Alice :; behind Alice as; said Alice .; . Alice\n",
      "thought; to Alice an; where Alice could; thought Alice ,; moment Alice\n",
      "appeared; ) Alice could; into Alice 's; . Alice was; ' Alice ventured;\n",
      "to Alice 's; . Alice did; upon Alice 's; ' Alice whispered; into Alice\n",
      "'s; ' Alice thought; ' Alice cautiously; ' Alice remarked; said Alice\n",
      ".; that Alice said; exclaimed Alice ,; ' Alice said; said Alice .;\n",
      "thought Alice .; said Alice sharply; to Alice 's; . Alice looked; to\n",
      "Alice ;; and Alice was; and Alice ,; to Alice ,; said Alice .; said\n",
      "Alice .; , Alice heard; leaving Alice alone; . Alice did; to Alice .;\n",
      "said Alice .; thought Alice ,; , Alice could; . Alice thought; . Alice\n",
      "was; ' Alice asked; poor Alice ,; interrupted Alice .; before Alice\n",
      "could; said Alice ;; said Alice ,; said Alice indignantly; said Alice\n",
      ";; inquired Alice .; ' Alice ventured; said Alice doubtfully; ' Alice\n",
      "did; said Alice .; said Alice ,; exclaimed Alice .; to Alice ,; '\n",
      "Alice went; at Alice ,; said Alice ); ( Alice began; said Alice .; at\n",
      "Alice .; said Alice timidly; said Alice .; round Alice ,; said Alice\n",
      ",; said Alice ,; ' Alice replied; said Alice ,; said Alice .; . Alice\n",
      "was; ' Alice looked; ' Alice asked; said Alice ,; said Alice in; said\n",
      "Alice .; said Alice a; So Alice began; over Alice .; thought Alice ;;\n",
      "' Alice said; ' Alice said; ' Alice did; and Alice was; ' Alice\n",
      "replied; taking Alice by; ' Alice panted; made Alice quite; . Alice\n",
      "had; thought Alice ,; ' Alice whispered; ' Alice began; . Alice could;\n",
      "thought Alice .; , Alice could; moment Alice felt; said Alice very;\n",
      "said Alice more; thought Alice .; thought Alice .; and Alice guessed;\n",
      "' Alice watched; . Alice 's; cried Alice ,; at Alice as; . Alice\n",
      "looked; to Alice .; said Alice .; said Alice .; ' Alice could; at\n",
      "Alice .; said Alice .; said Alice :; said Alice .; said Alice .; said\n",
      "Alice ,; said Alice .; said Alice loudly; said Alice .; said Alice ,;\n",
      ", Alice dear; said Alice ,; So Alice got; little Alice and; little\n",
      "Alice herself\n"
     ]
    }
   ],
   "source": [
    "regex = my_text.findall('<.*><[Aa]lice><.*>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is this useful?**\n",
    "\n",
    "A powerful use of keyword-in-context displays comes from a recent Boston Globe piece cataloging [words spoken by suspects at or around their arrest](http://apps.bostonglobe.com/graphics/2016/04/arresting-words/).\n",
    "\n",
    "Here is a screenshot of the piece showing mentions of the word \"phone\":\n",
    "\n",
    "![](imgs/arrest_phones.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the most basic, but still insightful metric we could get from an entire document a count of unique tokens.\n",
    "\n",
    "This gives us a sense of the vocabulary size and repetition of words. \n",
    "\n",
    "Let's make a function to count the number of times each token appears in our document and use it on our word tokens. \n",
    "\n",
    "Sound fun? Great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# input: list of tokens\n",
    "# returns: dict of counts\n",
    "def get_counts(tokens):\n",
    "    return Counter(tokens)\n",
    "\n",
    "\n",
    "# input: dictionary of tokens:counts\n",
    "# returns: sorted list of (token, count)\n",
    "def sort_counts(counts):\n",
    "    return sorted(counts.items(), key=lambda count: count[1], reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get_sorted_counts just runs get_counts and sort_counts together.\n",
    "\n",
    "# input: list of tokens\n",
    "# returns: list of (token, count) values \n",
    "#  sorted with most used counts on top.\n",
    "def get_sorted_counts(tokens):\n",
    "    return sort_counts(get_counts(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# how many counts does Alice have?\n",
    "counts = get_counts(tokens)\n",
    "counts['Alice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can also use `get_sorted_counts` to see most used tokens in our document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 2418),\n",
       " ('the', 1516),\n",
       " (\"'\", 1311),\n",
       " ('.', 981),\n",
       " ('and', 757),\n",
       " ('to', 717),\n",
       " ('a', 614),\n",
       " ('I', 543),\n",
       " ('it', 513),\n",
       " ('she', 507),\n",
       " ('of', 496),\n",
       " ('said', 456),\n",
       " ('!', 450),\n",
       " ('Alice', 394),\n",
       " ('was', 362),\n",
       " ('in', 351),\n",
       " ('you', 337),\n",
       " ('that', 267),\n",
       " ('--', 264),\n",
       " ('her', 243)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# what are the top used tokens in our data?\n",
    "sorted_counts = get_sorted_counts(tokens)\n",
    "sorted_counts[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Filtering & Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation\n",
    "\n",
    "There is a lot of punctuation in that top list. Let's deal with that now. \n",
    "\n",
    "We can create a new function that strips out any tokens that are considered punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: list of tokens\n",
    "# input: string or list of tokens to remove\n",
    "# output: list of tokens with remove_tokens removed\n",
    "def remove_tokens(tokens, remove_tokens):\n",
    "    return [token for token in tokens if token not in remove_tokens]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~--''`\n"
     ]
    }
   ],
   "source": [
    "# we can get a starting point for punctuation from python string\n",
    "from string import punctuation\n",
    "\n",
    "# augmenting the base set - to better fit this data.\n",
    "punc = punctuation + \"--''`\"\n",
    "print(punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAPTER',\n",
       " 'I',\n",
       " 'Down',\n",
       " 'the',\n",
       " 'Rabbit-Hole',\n",
       " 'Alice',\n",
       " 'was',\n",
       " 'beginning',\n",
       " 'to',\n",
       " 'get']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_punc_tokens = remove_tokens(tokens, punc)\n",
    "no_punc_tokens[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Case\n",
    "\n",
    "You also notice that some of our words start with a capital letter and some don't. We can normalize all our tokens by using a function to convert them all to lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: list of tokens\n",
    "# output: list of tokens with every token having only lowercase letters.\n",
    "def lowercase(tokens):\n",
    "    return [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets combine the two to **normalize** our tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1617),\n",
       " ('and', 810),\n",
       " ('to', 720),\n",
       " ('a', 631),\n",
       " ('she', 545),\n",
       " ('i', 543),\n",
       " ('it', 540),\n",
       " ('of', 499),\n",
       " ('said', 462),\n",
       " ('alice', 396),\n",
       " ('was', 367),\n",
       " ('you', 359),\n",
       " ('in', 358),\n",
       " ('that', 284),\n",
       " ('as', 256),\n",
       " ('her', 248),\n",
       " (\"n't\", 217),\n",
       " ('at', 209),\n",
       " (\"'s\", 200),\n",
       " ('on', 192)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "normalized_tokens = remove_tokens(lowercase(tokens), punc)\n",
    "\n",
    "# run sort again\n",
    "sorted_counts = get_sorted_counts(normalized_tokens)\n",
    "\n",
    "sorted_counts[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "\n",
    "We have a list of top words - but they mostly look pretty boring. Of course 'the' is the most used word here - because it is the most used word everywhere! \n",
    "\n",
    "The rationale behind doing this is that typically these words add little value towards extracting meaning from text - as they are used so frequently in normal text. So we can just take them out! \n",
    "\n",
    "But be careful about this filtering process! With this we are again removing data, so it’s good to just pause and make sure this reduction is useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nigel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stops = stopwords.words('english')\n",
    "\n",
    "# check out what they look like.\n",
    "print(len(stops))\n",
    "print(stops[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 462),\n",
       " ('alice', 396),\n",
       " (\"n't\", 217),\n",
       " (\"'s\", 200),\n",
       " ('little', 128),\n",
       " ('one', 99),\n",
       " ('would', 90),\n",
       " ('know', 87),\n",
       " ('could', 86),\n",
       " ('like', 85),\n",
       " ('went', 83),\n",
       " ('queen', 75),\n",
       " ('thought', 74),\n",
       " ('time', 68),\n",
       " ('see', 67),\n",
       " ('king', 63),\n",
       " (\"'m\", 59),\n",
       " ('turtle', 59),\n",
       " ('began', 58),\n",
       " (\"'ll\", 57)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use remove_tokens to remove stop words\n",
    "filtered_normalized_tokens = remove_tokens(normalized_tokens, stops)\n",
    "\n",
    "sorted_counts = get_sorted_counts(filtered_normalized_tokens)\n",
    "\n",
    "sorted_counts[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red\" class=\"exercise\">Your Turn</h3>\n",
    "\n",
    "Data analysis is 80% data cleaning right? So let's keep cleaning!\n",
    "\n",
    "Notice we still have a lot of odd tokens that are the ends of concatenations. \n",
    "\n",
    "Develop a function to remove these tokens from our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 462),\n",
       " ('alice', 396),\n",
       " ('little', 128),\n",
       " ('one', 99),\n",
       " ('would', 90),\n",
       " ('know', 87),\n",
       " ('could', 86),\n",
       " ('like', 85),\n",
       " ('went', 83),\n",
       " ('queen', 75),\n",
       " ('thought', 74),\n",
       " ('time', 68),\n",
       " ('see', 67),\n",
       " ('king', 63),\n",
       " ('turtle', 59),\n",
       " ('began', 58),\n",
       " ('hatter', 56),\n",
       " ('mock', 56),\n",
       " ('quite', 55),\n",
       " ('gryphon', 55)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Your code:\n",
    "# Finish this function to filter these word fragments\n",
    "# Hint: \"'\" not in token  \n",
    "# ^ this will return true if the ' character isn't in the string 'token'\n",
    "def remove_word_fragments(tokens):\n",
    "    tokens = [token for token in tokens if \"'\" not in token]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "# Then we will call it to further filter our words.\n",
    "more_filtered_normalized_tokens = remove_word_fragments(filtered_normalized_tokens)\n",
    "\n",
    "sorted_counts = get_sorted_counts(more_filtered_normalized_tokens)\n",
    "sorted_counts[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is a transformation process that seeks to convert words to their \"base\" or root forms. \n",
    "\n",
    "So, for example, **housing**, **housed**, and **house** could all be collapsed to the root **hous**. \n",
    "\n",
    "The idea is to collapse these similar words into a single token representation in the document. \n",
    "\n",
    "This isn't great for visualizations - but can be useful when counting, comparing, or developing other metrics. \n",
    "\n",
    "Here we use the `PorterStemmer` which implements one of the most popular stemming algorithms for English-language documents. \n",
    "\n",
    "(This algorithm is called the \"Porter Stemmer\" because it was developed by a Dr. [Martin Porter](https://en.wikipedia.org/wiki/Martin_Porter) in 1980.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hous', 'hous', 'hous', 'mous', 'mousi', 'mousey']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "some_words = ['house', 'housing', 'housed', 'mouse', 'mousy', 'mousey']\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in some_words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting keywords is a very important tasks when working with text. Keywords start to answer the question \"What is this document about\". \n",
    "\n",
    "They can work to summarize a document, providing a starting point for topic analysis. Keywords can also show what makes a document unique.\n",
    "\n",
    "But what are keywords? Keywords (or keyphrases) can be defined as _distinctive_ words or phrases in a document. Obviously, this definition relies on what we mean by distinctive. As you might expect, there are many methods for conjuring up distinctive phrases from text. \n",
    "\n",
    "Here we will look at two approaches:\n",
    "\n",
    "* comparing terms to other terms within the document with collocations.\n",
    "* comparing words to other words from an external corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Collocations\n",
    "\n",
    "A **collocation** is a set of words that occur together more often then chance. These expressions consist of two or more words and can sometimes correspond to some conventional way of saying something.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has a handy **Collocation Finder** class that can help us here. \n",
    "\n",
    "We create a new bigram finder by passing in our tokens to the `BigramCollocationFinder.from_words()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(normalized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But to get \"good\" common bigrams out, we need to define a metric to sort bigrams so that interesting collocations can bubble to the top. \n",
    "\n",
    "_So what should this sorting metric be?_\n",
    "\n",
    "NLTK has a few built in ones to choose from, let's look at one. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been using word frequency a lot to examine our data processing procedures, what if we just used counts (frequencies) of bigrams? \n",
    "\n",
    "More interesting collocations should appear more often - right?\n",
    "\n",
    "Lets use the `raw_freq` meausure to score bigrams based on counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('said', 'the'), 0.007674230740985533),\n",
       " (('of', 'the'), 0.004700007343761475),\n",
       " (('said', 'alice'), 0.004259381655283836),\n",
       " (('in', 'a'), 0.0035617243151942423),\n",
       " (('and', 'the'), 0.002900785782477785),\n",
       " (('in', 'the'), 0.002900785782477785),\n",
       " (('it', 'was'), 0.002680472938238966),\n",
       " (('the', 'queen'), 0.00253359770874642),\n",
       " (('to', 'the'), 0.00253359770874642),\n",
       " (('the', 'king'), 0.002276566057134464)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# built in bigram metrics are in here\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "\n",
    "# we call score_ngrams on the finder to produce a sorted list\n",
    "# of bigrams. Each comes with its score from the metric, which\n",
    "# is how they are sorted. \n",
    "finder.score_ngrams(bigram_measures.raw_freq)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What? These collocations are **BORING**\n",
    "\n",
    "As you might guess by now, \n",
    "\n",
    "It would be better to use a scoring function that took into account the unusual-ness of the terms into account.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Log likelihood\n",
    "\n",
    "A better method could use the _likelihood_ of two words occurring together to generate a distance metric. \n",
    "\n",
    "Log likelihood is one such method. Log likelihood looks at the counts of events occuring together vs them occuring separately to try to tease out the difference between _surprise_ and _coincidence_. \n",
    "\n",
    "Here, the events are any two words appearing together. The calculation looks at how often words appear together vs the same words appearing not together to come up with likelihood scores. \n",
    "\n",
    "Fortunately, NLTK makes it super easy to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('mock', 'turtle'), 781.0958309926607),\n",
       " (('said', 'the'), 597.9725394724204),\n",
       " (('said', 'alice'), 505.47797818438784),\n",
       " (('i', \"'m\"), 468.5116913414714),\n",
       " (('march', 'hare'), 461.9215891322542),\n",
       " (('went', 'on'), 376.07074397098586),\n",
       " (('do', \"n't\"), 372.7069673845066),\n",
       " (('the', 'queen'), 351.3982433644536),\n",
       " (('the', 'king'), 342.27732696590454),\n",
       " (('in', 'a'), 337.9247961471988),\n",
       " (('the', 'gryphon'), 284.0399154326837),\n",
       " (('the', 'mock'), 277.94479670005336),\n",
       " (('a', 'little'), 276.1051075393118),\n",
       " (('the', 'hatter'), 266.9307751748064),\n",
       " (('ca', \"n't\"), 264.4268785944248),\n",
       " (('you', 'know'), 258.02012533760467),\n",
       " (('white', 'rabbit'), 257.569682778391),\n",
       " (('she', 'had'), 247.48309230433605),\n",
       " (('wo', \"n't\"), 234.68843587296746),\n",
       " (('it', 'was'), 226.63566921399)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.score_ngrams(bigram_measures.likelihood_ratio)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an improvement over raw frequencies in terms of what we would consider 'interesting' phrases.\n",
    "\n",
    "What if we combined this scoring with stop word removal? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red\" class=\"exercise\">Your Turn</h3>\n",
    "\n",
    "Create a Bigram Collocation Finder and run it on the tokens with stop words filtered out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('mock', 'turtle'), 688.746264412505),\n",
       " (('march', 'hare'), 410.8359988969317),\n",
       " (('said', 'alice'), 360.53577375712536),\n",
       " (('white', 'rabbit'), 221.34355890809772),\n",
       " (('join', 'dance'), 131.39668847455897),\n",
       " (('minute', 'two'), 114.41513121542064),\n",
       " (('said', 'king'), 106.06681716381577),\n",
       " (('soo', 'oop'), 97.42210864977716),\n",
       " (('beg', 'pardon'), 94.16637479191395),\n",
       " (('thought', 'alice'), 85.96838770645795),\n",
       " (('golden', 'key'), 85.9653557965685),\n",
       " (('oh', 'dear'), 82.55450801319292),\n",
       " (('said', 'caterpillar'), 82.06671087946238),\n",
       " (('set', 'work'), 75.04762680556932),\n",
       " (('good', 'deal'), 72.93801186820265),\n",
       " (('evening', 'beautiful'), 72.63521157132631),\n",
       " (('kid', 'gloves'), 72.63521157132631),\n",
       " (('beau', 'ootiful'), 72.02021490879194),\n",
       " (('yer', 'honour'), 72.02021490879194),\n",
       " (('play', 'croquet'), 71.8021706349689)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Your code\n",
    "# create a BigramCollocationFinder from the stopword filtered tokens\n",
    "# use your choice of scoring metric to score bigrams. \n",
    "# You can use tab completion on the bigram_measures to see other metrics too\n",
    "\n",
    "Bigram = BigramCollocationFinder.from_words(more_filtered_normalized_tokens)\n",
    "Bigram.score_ngrams(bigram_measures.raw_freq)[:10]\n",
    "\n",
    "tabCompletion = Bigram.score_ngrams(bigram_measures.likelihood_ratio)[0:20]\n",
    "tabCompletion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Reading**\n",
    "\n",
    "* [Surprise and Coincidence](http://tdunning.blogspot.com/2008/03/surprise-and-coincidence.html)\n",
    "* [Stereotropes likelihood](http://stereotropes.bocoup.com/about#gender-association)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing to the English Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Term Frequency - Inverse Document Frequency\n",
    "\n",
    "We have seen that word counts, also known as **term frequency** is not a very useful metric when attempting to find interesting words or phrases. Even with stopwords removed, the most frequent terms in a document are typically pretty boring. \n",
    "\n",
    "The core problem really is that not all words in a document carry the same weight in terms of significance or pertinence. \n",
    "\n",
    "The words \"the\" and \"turtle\" do not provide the same amount of information. \"the\" is a word used all the time in english. If a document has a bunch of \"the\"s in it, that really doesn't tell us anything. But, if you see lots of \"turtle\"s, you might want to pay attention.\n",
    " \n",
    "\n",
    "TF-IDF tries to capture this idea: _words that don’t occur in often in communication but which occur a lot in your document are important to the document’s content._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it would be nice to know the frequency of use of all words in all communications in a language, practically speaking - that is impossible.\n",
    "\n",
    "So a **corpus** or collection of documents is typically used as a proxy to what general communcation looks like. \n",
    "\n",
    "We compare our document against a collection of documents to find out what words or phrases make our document unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Calculation\n",
    "\n",
    "But what is TF-IDF exactly, and how do we calculate it? Let's get to that now!\n",
    "\n",
    "We know **term frequency** is just the count of a particular token or term in a document. \n",
    "\n",
    "**document frequency** is defined as the count of **documents** that a particular token appears in. \n",
    "\n",
    "\n",
    "_Quick Example:_\n",
    "\n",
    "\n",
    "Say we have 3 documents in our corpus, and the token we are looking for is \"turtle\". \n",
    "\n",
    "* Doc 1 has turtle 3 times\n",
    "* Doc 2 has turtle 0 times\n",
    "* Doc 3 has turtle 12 times\n",
    "\n",
    "Those counts right there are the term frequencies for the particular term in particular documents. \n",
    "\n",
    "The document frequency of \"turtle\" would be _2_ - as two of the three of the documents in the corpus contain turtle. \n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "To get the **inverse document frequency**, we simply take \n",
    "\n",
    "```\n",
    "1 / document frequency\n",
    "```\n",
    "\n",
    "So the full calculation is just:\n",
    "\n",
    "```\n",
    "term frequency * (1 / document frequency)\n",
    "```\n",
    "\n",
    "Seems too simple to be true - and it is!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The calculation for **IDF** we will use is:\n",
    "\n",
    "```\n",
    "log(1 + N / document frequency)\n",
    "```\n",
    "\n",
    "Where `N` is the number of documents in the corpus.\n",
    "\n",
    "\n",
    "I said the above calculation was too easy, and it is. There are a few optimizations to IDF that are typically applied.\n",
    "\n",
    "* We want to make sure we aren't dividing by zero.\n",
    "\n",
    "* The linear weighting is a bit heavy handed. A rare token found in two documents is most likely not half as interesting as a token found in just one document. \n",
    "\n",
    "* We probably want to normalize based on corpus size.\n",
    "\n",
    "\n",
    "Here are some great graphs from a [good TF-IDF explaination](https://porganized.com/2016/03/09/term-weighting-for-humanists/) that show how different IDF calculations look as a function of document frequency. \n",
    "\n",
    "![](imgs/idf-curve1.png)\n",
    "\n",
    "For the term frequency calculation, **TF**, we will also divide by the number of tokens in the document. \n",
    "\n",
    "This will make it so that longer documents are not unfairly boosted by their token counts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Code\n",
    "\n",
    "Enough talking, let's get to coding!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the **brown** corpus, which is a set of documents broken up into differnet categories. \n",
    "\n",
    "(and one of the first [computer readable corpula](http://www.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\nigel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the brown corpus package to our notebook\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "# Here we can see the categories used to splitup the articles in the dataset.\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate TF-IDF we will need to calculate the document frequency for any token. This means we should keep each document in our corpus separate (as opposed to mashing it all together in one big bag of words) so we know if a token occurs in a particular document. \n",
    "\n",
    "Here is one way I found to keep the tokens for each document in the brown corpus in a separate list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# our corpus tokens will be an list of lists.\n",
    "news_tokens = []\n",
    "\n",
    "# Iterate over the filenames for each document in brown. \n",
    "# We are using only those documents in the 'news' category to speed things up.\n",
    "for filename in brown.fileids(categories='news'):\n",
    "    # Do our processing on the corpus documents to keep things consistent. \n",
    "    bwords = brown.words(fileids=filename)\n",
    "    processed_news = lowercase(bwords)\n",
    "    processed_news = remove_tokens(processed_news, punc)\n",
    "    processed_news = remove_tokens(processed_news, stops)\n",
    "\n",
    "    news_tokens.append(processed_news)\n",
    "\n",
    "# this is the number of documents in our corpus\n",
    "len(news_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create functions for each portion of the TF-IDF calculation. \n",
    "\n",
    "Lets start with **term frequency**. Assuming we have a dictionary of counts for each token in a document, the calculation becomes simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: list of tokens\n",
    "# returns: dict of counts\n",
    "def get_counts(tokens):\n",
    "    return Counter(tokens)\n",
    "\n",
    "# input token: the token we are looking at\n",
    "# input counts: token count dictionary for one document\n",
    "def term_frequency(token, counts):\n",
    "    '''Calculate term frequency for a particular token in a particular document'''\n",
    "    return counts[token] / float(len(counts.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this function a bit with our existing tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4812698412698413\n",
      "0.12507936507936507\n",
      "0.00031746031746031746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3150"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = get_counts(tokens)\n",
    "\n",
    "\n",
    "\n",
    "print(term_frequency('the', counts))\n",
    "print(term_frequency('Alice', counts))\n",
    "print(term_frequency('walrus', counts))\n",
    "\n",
    "len(counts.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, let's make a `document_frequency` function. \n",
    "\n",
    "Again, Document Frequency refers to the number of documents that contain a particular token. \n",
    "\n",
    "We will provide a token and our list of lists that stores our corpus. We want out how many documents in that corpus contain the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input token: a token to search the corpora for\n",
    "# input corpus_tokens: list of lists of tokens.\n",
    "#  One list for each document in the corpora.\n",
    "# output: number of documents in corpora that contain the token.\n",
    "def document_frequency(token, corpus_tokens):\n",
    "    '''Returns number of times a token appears in a set of documents'''\n",
    "    doc_count = 0\n",
    "    for tokens in corpus_tokens:\n",
    "        \n",
    "        if token in tokens:\n",
    "            doc_count += 1\n",
    "            \n",
    "    return doc_count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 7, 3, 2, 0]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing document_frequency on some bizness and not so bizness words. \n",
    "test_words = [\"business\", \"account\", \"welcome\", \"alice\", \"stillsuit\"]\n",
    "[document_frequency(token, news_tokens) for token in test_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add our optimizations to this metric by creating a `inverse_doc_frequency` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# input: token: token we are analyzing \n",
    "# input: corpus_tokens: list of lists of tokens.\n",
    "#  One list for each document in the corpora.\n",
    "def inverse_doc_frequency(token, corpus_tokens):\n",
    "    return math.log(1 +  len(corpus_tokens) / (document_frequency(token, corpus_tokens) + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world\n",
      "28\n",
      "0.923163611161917\n",
      "house\n",
      "29\n",
      "0.9028677115420144\n",
      "alice\n",
      "2\n",
      "2.751535313041949\n",
      "hatter\n",
      "0\n",
      "3.8066624897703196\n"
     ]
    }
   ],
   "source": [
    "for token in [\"world\", \"house\", \"alice\", \"hatter\"]:\n",
    "    print(token)\n",
    "    print(document_frequency(token, news_tokens))\n",
    "    print(inverse_doc_frequency(token, news_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And now the big finish!**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input document_tokens: a list of tokens that represent a document\n",
    "# input corpus_tokens: list of lists of tokens.\n",
    "#  One list for each document in the corpora.\n",
    "# output: list of (token, tf-idf) values for each unique token in document_tokens\n",
    "def tf_idf(document_tokens, corpus_tokens):\n",
    "\n",
    "    \n",
    "    # Get our token frequencies for all the unique tokens in our document\n",
    "    token_counts = get_counts(document_tokens)\n",
    "    \n",
    "    # iterate through these tokens and calculate the tf-idf\n",
    "    tfidfs = {}\n",
    "    for token in token_counts.keys():\n",
    "        \n",
    "        tf = term_frequency(token, token_counts)\n",
    "        idf = inverse_doc_frequency(token, corpus_tokens)\n",
    "        \n",
    "        tfidfs[token] = tf * idf\n",
    "        \n",
    "    \n",
    "    return tfidfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ~~~WARNING~~~\n",
    "# this takes a while to run!\n",
    "token_tf_idfs = tf_idf(filtered_normalized_tokens, news_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alice', 0.4099352836586199),\n",
       " (\"n't\", 0.31077718595942794),\n",
       " (\"'s\", 0.2864305861377216),\n",
       " ('said', 0.12895976420804076),\n",
       " ('queen', 0.08847331309055163),\n",
       " (\"'m\", 0.08449702291062786),\n",
       " ('turtle', 0.08449702291062786),\n",
       " (\"'ll\", 0.08163271704925065),\n",
       " (\"'and\", 0.08020056411856204),\n",
       " ('hatter', 0.08020056411856204),\n",
       " ('mock', 0.08020056411856204),\n",
       " (\"'it\", 0.07876841118787344),\n",
       " ('gryphon', 0.07876841118787344),\n",
       " (\"'you\", 0.073039799465119),\n",
       " (\"'ve\", 0.06301472895029875),\n",
       " ('duchess', 0.06015042308892153),\n",
       " ('dormouse', 0.057286117227544314),\n",
       " (\"'but\", 0.055853964296855706),\n",
       " ('rabbit', 0.055443276203412356),\n",
       " (\"'re\", 0.0544218113661671)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sorted_token_tf_idfs = sort_counts(token_tf_idfs)\n",
    "sorted_token_tf_idfs[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Summarization\n",
    "\n",
    "**Summarization** is the idea of collapsing a document down to a quick digestable chunk or summary. \n",
    "\n",
    "\n",
    "This is especially interesting for more newsy and technical documents where an \"abstract-like\" summary could be enough for a reader to decide if it is worth reading the document in full. \n",
    "\n",
    "A [simple but interesting algorithm]((http://courses.ischool.berkeley.edu/i256/f06/papers/luhn58.pdf) was described in 1957.\n",
    "\n",
    "Check out this [great interactive explaination](http://www.fastforwardlabs.com/luhn/) of this algorithm.\n",
    "\n",
    "Here is the basic idea:\n",
    "\n",
    "* Take a document and remove stop words.\n",
    "* Pick the top X most frequent words, where X is 5 or so.\n",
    "* Rank sentences in the document based on how many times these most frequent words appear in the sentence.\n",
    "* Take the top 3 or 4 sentences as the summary. \n",
    "\n",
    "Most of these pieces we have already, we just need to put them together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Reading:**\n",
    "\n",
    "* [Intro to Keyphrase Extraction](http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/)\n",
    "* [Term Weighting for Humanists](https://porganized.com/2016/03/09/term-weighting-for-humanists/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
